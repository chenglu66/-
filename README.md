# regression
用线性回归找到最佳拟合曲线
回归一般就是拟合，即用数据的来拟合结果，所以就有一个问题怎么判别拟合好坏，怎么去拟合，
线性回归就是变量之间的线性组合。那么误差的好坏用最小二乘来衡量。感觉没什么了，
当问题欠拟合时即选取的模型不足以刻画整体数据。因为模型可能比较复杂，但是强行提高模型复杂度可能会过拟合
常用的办法是局部加权线性回归。选择与这个点相近的点而不是所有的点做线性回归。
就好像SVM分类器一样，就是我用需要点而不是所有点来预测数据，因为一些点可能和改点无关，比如马尔可夫链。所以用最相关的点来预测是最好的。
而相关怎么定义，定然是距离信息。说白了就是距离近的加权和的系数大点
基于这个思想，便产生了局部加权线性回归算法。在这个算法中，其他离一个点越近，权重越大，对回归系数的贡献就越多。
下面就是怎么刻画这个信息。这个我是想不出来，不过每个点都要给权重，距离远就小，距离近就大，负指数函数满足，但我还要保证可调，所以用高斯核
通过高斯核函数可以知道，k值越大，用于训练的点也会多些。越小用于训练的点也会少些。
可以想象这种方法得到的回归模型效果非常好，但是计算起来非常复杂，特别是数据比较多的时候。
当然这样想加入引入线性回归很容易过拟合，因为要求的指标是最小二乘，解决办法正则化，最小二乘相当于最大似然概率。
怎么衡量预测的好坏，标称可以用错误率来区分，回归问题可以用相关系数。
常规回归：
![image](https://github.com/chenglu66/regression/blob/master/figure_1-1.png)
局部加权回归：
当k=1时：
![image](https://github.com/chenglu66/regression/blob/master/k=1.png)
当k=0.01时：
![image](https://github.com/chenglu66/regression/blob/master/k=0.1.png)
当k=0.003时
![image](https://github.com/chenglu66/regression/blob/master/k=0.003.png)
此时相关系数为0.9999
毫无疑问过拟合了，解决过拟合方法，加正则L1正则和L2正则
L1正则加入是把部分系数变成0，l2是把系数变小，总之都是能更好的对待噪声数据。
下面是方差偏差权衡，方差是模型差异，偏差时模型预测值和数据之间的差异，但是方差怎么度量
就是先拿100组数据计算下回归系数，然后再拿100组系数计算回归系数，系数之间差异就是模型方差的反映。
因此高方差意味着对噪声很敏感了就是过拟合，高偏差就是差距太大了，这时可能是算法没收敛，或者数据样本不够多，或者是模型本身不好
我用正则的化虽然可能增大偏差，但是减小了方差。其实正则相当于确定了一个先验概率。
下面没啥说的就是运算了，看一个具体的例子：
预估玩具的价格：那么玩具价格和哪些因素有关，怎么衡量。我靠这数据不知道怎么来的，反正

